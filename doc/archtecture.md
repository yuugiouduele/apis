# 02_architecture.md
## å› æœæ§‹é€ çš„ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ â€• ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«è¨­è¨ˆã¨ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼

---

### I. ç›®çš„ã¨è¨­è¨ˆæ–¹é‡

æœ¬ç« ã§ã¯ `/docs/01_theory.md` ã§å®šç¾©ã—ãŸç†è«–ãƒ¢ãƒ‡ãƒ«ã‚’ã€
å®Ÿè£…å¯èƒ½ãªãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«æ§‹é€ ã¨ã—ã¦å…·ä½“åŒ–ã™ã‚‹ã€‚

æœ¬ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®æŒ‡é‡ã¯ä»¥ä¸‹ã®é€šã‚Šï¼š

1. **å› æœæ€§ã‚’ç¬¬ä¸€ç´šæ§‹é€ ã¨ã—ã¦æ‰±ã†ã€‚**  
   å„å±¤ã¯å˜ãªã‚‹ãƒ‡ãƒ¼ã‚¿å¤‰æ›ã§ãªãã€Œå› æœçš„å†™åƒï¼ˆCausal Mappingï¼‰ã€ã§ã‚ã‚‹ã€‚

2. **å¹¾ä½•å­¦çš„é€£çµæ€§ã‚’æ˜ç¤ºçš„ã«å°å…¥ã™ã‚‹ã€‚**  
   ãƒ‡ãƒ¼ã‚¿ç©ºé–“ã‚’æ ¼å­çš„ãƒˆãƒãƒ­ã‚¸ãƒ¼ã«å¤‰æ›ã—ã€Attentionã‚’å±€æ‰€å¹¾ä½•æ§‹é€ ã«åˆ¶ç´„ã™ã‚‹ã€‚

3. **å­¦ç¿’åŠ¹ç‡ã‚’è‡ªå·±æœ€é©åŒ–ã™ã‚‹ã€‚**  
   è‡ªå¾‹è¨ˆç®—åˆ¶å¾¡å±¤ï¼ˆMeta Utility Layerï¼‰ãŒå‹¾é…ä¼æ¬ã®ã€Œæ„å‘³ã€ã‚’è©•ä¾¡ã—ã€
   ç„¡é§„ãªè¨ˆç®—ã‚’è‡ªç™ºçš„ã«æŠ‘åˆ¶ã™ã‚‹ã€‚

---

### II. ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ä¸€è¦§

| ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«å | ç›®çš„ | ä¸»ãªå…¥åŠ›/å‡ºåŠ› | å®Ÿè£…ã‚¯ãƒ©ã‚¹ä¾‹ |
|---------------|------|----------------|----------------|
| `CausalEncoder` | å› æœãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æŠ½å‡ºï¼ˆæ½œåœ¨å› æœè¡¨ç¾ï¼‰ | X â†’ z | `CausalFNN` |
| `LatticeEncoder` | å› æœã‚°ãƒ©ãƒ•æ§‹é€ ã®æ ¼å­åŒ–ã¨GNNä¼æ’­ | z â†’ H_lattice | `CausalGNN` |
| `CausalAttention` | å› æœè·é›¢ã«åŸºã¥ãAttentioné‡ã¿ä»˜ã‘ | H_lattice â†’ A_causal | `CausalTransformer` |
| `MetaUtilityLayer` | è¨ˆç®—è³‡æºæœ€é©åŒ–ã¨å ±é…¬åˆ¶å¾¡ | A_causal â†’ O_final | `MetaController` |
| `LossIntegrator` | å„å±¤ã®å‹¾é…ã¨æ­£å‰‡åŒ–ã®çµ±åˆ | å„L â†’ L_total | `LossManager` |

---

### III. å…¨ä½“ãƒ•ãƒ­ãƒ¼

```java
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚        Input Data X        â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   1. Causal Encoder (FNN)  â”‚
    â”‚  - æ½œåœ¨å› æœå¤‰æ•° z ã‚’æŠ½å‡º     â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  2. Lattice Encoder (GNN)  â”‚
    â”‚  - å› æœæ ¼å­æ§‹é€ ã‚’ç”Ÿæˆ        â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   3. Causal Attention      â”‚
    â”‚  - RoPEã‚’å› æœè·é›¢ã«å¤‰æ›     â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ 4. Meta Utility Layer      â”‚
    â”‚ - è¨ˆç®—åŠ¹ç‡æœ€é©åŒ–ï¼ˆu_iåˆ¶å¾¡ï¼‰â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ 5. Loss Integrator         â”‚
    â”‚ - L_task + Î»L_causal ç­‰    â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â†“
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚  Output   â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### IV. å„ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®è©³ç´°è¨­è¨ˆ

#### 1. CausalEncoder (FNN)
**å½¹å‰²:**  
å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ X ã‹ã‚‰æ½œåœ¨å› æœãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ z ã‚’æŠ½å‡ºã™ã‚‹ã€‚

**æ§‹é€ :**
```python
z = f_theta(X)
Y_hat = g_phi(z, T)
```

å†…éƒ¨è¦ç´ :

æ¨™æº–FNN + LayerNorm

Dropout + causal mask (ä»‹å…¥Tã®å½±éŸ¿é™¤å»)

å‡ºåŠ›å±¤ã§æ½œåœ¨è¡¨ç¾ z ã‚’æ­£è¦åŒ– (z / ||z||)

æ‡¸å¿µç‚¹:

å‹¾é…æµãŒå¼±ã„å ´åˆã€zãŒç„¡æ„å‘³åŒ–ã™ã‚‹ã€‚

Î»_causal ã«ã‚ˆã‚Šé‡ã¿ã‚’å¼·åˆ¶çš„ã«æŒãŸã›ã‚‹ã€‚

2. LatticeEncoder (GNN)
å½¹å‰²:
z ã‚’æ ¼å­çš„ãƒˆãƒãƒ­ã‚¸ãƒ¼ã«æŠ•å½±ã—ã€å› æœçš„é€£çµã‚’æ˜ç¤ºåŒ–ã™ã‚‹ã€‚

ä¸»è¦å¼:

ğ»
ğ‘–
(
ğ‘™
+
1
)
=
ğœ
(
âˆ‘
ğ‘—
âˆˆ
ğ‘
(
ğ‘–
)
1
ğ‘‘
ğ‘–
ğ‘‘
ğ‘—
ğ‘Š
(
ğ‘™
)
ğ»
ğ‘—
(
ğ‘™
)
)
H 
i
(l+1)
â€‹
 =Ïƒ 
â€‹
  
jâˆˆN(i)
âˆ‘
â€‹
  
d 
i
â€‹
 d 
j
â€‹
 
â€‹
 
1
â€‹
 W 
(l)
 H 
j
(l)
â€‹
  
â€‹
 
ã“ã“ã§ N(i) ã¯å› æœéš£æ¥ãƒãƒ¼ãƒ‰é›†åˆ

ğ‘‘
ğ‘–
d 
i
â€‹
 ï¼šãƒãƒ¼ãƒ‰æ¬¡æ•°

è¨­è¨ˆæŒ‡é‡:

ã‚¨ãƒƒã‚¸é‡ã¿ã‚’å› æœè·é›¢ 
ğ‘‘
ğ‘
ğ‘
ğ‘¢
ğ‘ 
ğ‘
ğ‘™
d 
causal
â€‹
  ã«åŸºã¥ã„ã¦ã‚¹ãƒ‘ãƒ¼ã‚¹åŒ–

ãƒãƒ¼ãƒ‰æ›´æ–°ã¯å‹¾é…å®‰å®šåŒ–ã®ãŸã‚æ®‹å·®çµåˆ

3. CausalAttention (Transformer Block)
å½¹å‰²:
å› æœçš„RoPEã‚’çµ„ã¿è¾¼ã¿ã€Attentioné‡ã¿ã‚’å› æœçš„é€£çµã«å¤‰æ›ã€‚

å¼:

ğ´
ğ‘–
ğ‘—
=
Softmax
(
ğ‘„
ğ‘–
â‹…
ğ¾
ğ‘—
âŠ¤
ğ‘‘
+
ğ›¾
ğ‘“
(
ğ‘‘
ğ‘
ğ‘
ğ‘¢
ğ‘ 
ğ‘
ğ‘™
(
ğ‘–
,
ğ‘—
)
)
)
A 
ij
â€‹
 =Softmax( 
d
â€‹
 
Q 
i
â€‹
 â‹…K 
j
âŠ¤
â€‹
 
â€‹
 +Î³f(d 
causal
â€‹
 (i,j)))
RoPEæ‹¡å¼µ:

ğ‘„
ğ‘–
,
ğ¾
ğ‘—
=
RoPE
ğ‘
ğ‘
ğ‘¢
ğ‘ 
ğ‘
ğ‘™
(
ğ‘„
ğ‘–
,
ğ¾
ğ‘—
,
ğ‘‘
ğ‘
ğ‘
ğ‘¢
ğ‘ 
ğ‘
ğ‘™
)
Q 
i
â€‹
 ,K 
j
â€‹
 =RoPE 
causal
â€‹
 (Q 
i
â€‹
 ,K 
j
â€‹
 ,d 
causal
â€‹
 )
é€šå¸¸ã®ãƒˆãƒ¼ã‚¯ãƒ³ä½ç½® 
ğ‘š
m â†’ å› æœè·é›¢ 
ğ‘‘
ğ‘
ğ‘
ğ‘¢
ğ‘ 
ğ‘
ğ‘™
d 
causal
â€‹
  ã«ç½®æ›

Î¸é–¢æ•°ã‚’å­¦ç¿’å¯èƒ½ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŒ–

å‡ºåŠ›:

A_causalï¼šå› æœAttentionè¡¨ç¾

H_outï¼šæ®‹å·®æ¥ç¶šã«ã‚ˆã‚Šå®‰å®šåŒ–

4. MetaUtilityLayer
å½¹å‰²:
æ¼”ç®—ãƒ–ãƒ­ãƒƒã‚¯ã”ã¨ã®utilityã‚’å‹•çš„åˆ¶å¾¡ã€‚
è¨ˆç®—é‡ã¨æå¤±æ”¹å–„åº¦ã«åŸºã¥ã„ã¦å‹¾é…æµã‚’æŠ‘åˆ¶/ä¿ƒé€²ã€‚

å¼:

ğ‘¢
ğ‘–
=
ğœ
(
ğ›¼
Î”
ğ¿
ğ‘–
âˆ’
ğ›½
ğ¶
ğ‘–
)
u 
i
â€‹
 =Ïƒ(Î±Î”L 
i
â€‹
 âˆ’Î²C 
i
â€‹
 )
é«˜ã„ 
ğ‘¢
ğ‘–
u 
i
â€‹
 ï¼šè¨ˆç®—ã‚’å®Ÿè¡Œ

ä½ã„ 
ğ‘¢
ğ‘–
u 
i
â€‹
 ï¼šskip connectionæ´»æ€§

å®Ÿè£…æ¡ˆ:

```python
for block in model.blocks:
    delta_L = estimate_loss_reduction(block)
    u = torch.sigmoid(alpha * delta_L - beta * block.cost)
    if u > threshold:
        y = block(y)
```
æ³¨æ„ç‚¹:

ãƒ«ãƒ¼ãƒ—å†…ã§backpropã‚’é€šã™ãŸã‚ã€uã‚’å¾®åˆ†å¯èƒ½ã«ä¿ã¤ã€‚

ã‚³ã‚¹ãƒˆæ­£è¦åŒ–ãŒé‡è¦ï¼ˆC_iã‚’GPU FLOPSã§è¿‘ä¼¼å¯ï¼‰

5. LossIntegrator
å½¹å‰²:
å„æå¤±ã‚’çµ±åˆã—ã¦æœ€çµ‚çš„ãª L_total ã‚’è¨ˆç®—ã€‚

ğ¿
ğ‘¡
ğ‘œ
ğ‘¡
ğ‘
ğ‘™
=
ğ¿
ğ‘¡
ğ‘
ğ‘ 
ğ‘˜
+
ğœ†
1
ğ¿
ğ‘
ğ‘
ğ‘¢
ğ‘ 
ğ‘
ğ‘™
+
ğœ†
2
ğ¿
ğ‘”
ğ‘’
ğ‘œ
ğ‘š
ğ‘’
ğ‘¡
ğ‘Ÿ
ğ‘–
ğ‘
+
ğœ†
ğ‘Ÿ
ğ‘’
ğ‘”
ğ¿
ğ‘Ÿ
ğ‘’
ğ‘”
L 
total
â€‹
 =L 
task
â€‹
 +Î» 
1
â€‹
 L 
causal
â€‹
 +Î» 
2
â€‹
 L 
geometric
â€‹
 +Î» 
reg
â€‹
 L 
reg
â€‹
 
ã‚µãƒ–æ§‹æˆ:

L_task: CrossEntropy / MSE / CosineSim

L_causal: P(Y|do(T))å®‰å®šæ€§æå¤±

L_geo: ãƒãƒ¼ãƒ‰é€£çµå®‰å®šæ€§æå¤±

L_reg: L_DC + L_Dis + L_Stable

V. å­¦ç¿’ãƒ«ãƒ¼ãƒ—è¨­è¨ˆï¼ˆæ“¬ä¼¼ã‚³ãƒ¼ãƒ‰ï¼‰
```python
for epoch in range(EPOCHS):
    for X, Y, T in dataloader:
        # 1. forward
        z = causal_encoder(X)
        H = lattice_encoder(z)
        A = causal_attention(H, T)
        O = meta_utility(A)
        Y_hat = output_head(O)

        # 2. loss
        L_task = task_loss(Y_hat, Y)
        L_causal = causal_loss(Y_hat, Y, T)
        L_geo = geometric_loss(H)
        L_reg = regularization(z, Y)
        L_total = L_task + Î»1*L_causal + Î»2*L_geo + L_reg

        # 3. backward
        optimizer.zero_grad()
        L_total.backward()
        clip_grad_norm_(model.parameters(), 1.0)
        optimizer.step()
```
VI. å®‰å®šåŒ–ãŠã‚ˆã³å†ç¾æ€§ç®¡ç†
å†ç¾æ€§ç¢ºä¿

```python
torch.manual_seed(42)
```

ä¹±æ•°ãƒ»ãƒ‡ãƒ¼ã‚¿ã‚·ãƒ£ãƒƒãƒ•ãƒ«å›ºå®š

Dockerç’°å¢ƒåŒ–ã§ä¾å­˜æ€§å›ºå®š

å­¦ç¿’å®‰å®šåŒ–

Gradient Clipping (1.0)

Dynamic Î» scheduler

Warmup + CosineAnnealing LR

BatchNormã§å‹¾é…çˆ†ç™ºå›é¿

VII. è¨ˆç®—è¤‡é›‘åº¦ï¼ˆæ¦‚ç®—ï¼‰
éƒ¨åˆ†	ã‚ªãƒ¼ãƒ€ãƒ¼	ã‚³ãƒ¡ãƒ³ãƒˆ
FNN (CausalEncoder)	O(NÂ·dÂ²)	æ¨™æº–ç·šå½¢å¤‰æ›
GNN (LatticeEncoder)	O(EÂ·d)	ã‚¹ãƒ‘ãƒ¼ã‚¹åŒ–ã«ã‚ˆã‚Šè»½é‡åŒ–
Attention (Causal)	O(NÂ²Â·d)	causalè·é›¢è¿‘å‚ã®ã¿æ¡ç”¨ã§å‰Šæ¸›
MetaUtilityLayer	O(B)	ãƒ–ãƒ­ãƒƒã‚¯å˜ä½åˆ¶å¾¡
Total	â‰ˆ O(N log N)ï¼ˆè¿‘ä¼¼ï¼‰	å®Ÿç”¨ãƒ¬ãƒ™ãƒ«ã§å®‰å®š

VIII. å‡ºåŠ›ä»•æ§˜
O_final: å› æœçš„ç‰¹å¾´çµ±åˆè¡¨ç¾ï¼ˆNÃ—dï¼‰

L_total: æå¤±ç·å’Œ

u: å„ãƒ–ãƒ­ãƒƒã‚¯ã®utilityãƒ™ã‚¯ãƒˆãƒ«

logs: å‹¾é…ãƒ»Î»ãƒ»å®‰å®šæ€§ãƒ¡ãƒˆãƒªã‚¯ã‚¹ï¼ˆtensorboardå¯è¦–åŒ–å¯¾è±¡ï¼‰

IX. æ¬¡ç« 
æ¬¡ç«  /docs/03_training.md ã§ã¯ã€
å­¦ç¿’ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã€ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã€å†ç¾æ€§ç®¡ç†ã€
ãŠã‚ˆã³è©•ä¾¡åŸºæº–ã®è©³ç´°ã‚’è¨˜è¿°ã™ã‚‹ã€‚

